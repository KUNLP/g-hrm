import re
import string
import os
from typing import List, Union, Literal, Optional
from enum import Enum
import tiktoken
import openai
import qianfan
from openai import OpenAI
import time
import json
import multiprocessing
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path
from datetime import datetime
from collections import Counter

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

# .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
def load_env_file():
    """í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    env_file = Path(".env")
    if env_file.exists():
        with open(env_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key] = value

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_env_file()

from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain_community.chat_models import ChatOpenAI
from langchain_community.chat_models import QianfanChatEndpoint
from langchain_core.messages import SystemMessage, HumanMessage

# ì„í¬íŠ¸ ì˜¤ë¥˜ ì²˜ë¦¬
try:
    from src.graph_prompts import (
        GRAPH_DEFINITION, 
        enhanced_plan_generation_prompt, 
        enhanced_plan_generation_complex_prompt, 
        enhanced_plan_evaluation_prompt, 
        simplified_plan_evaluation_prompt, 
        enhanced_answer_evaluation_prompt, 
        reflect_graph_compound_and_plan_prompt, 
        graph_compound_and_plan_eval_prompt
    )
    from src.graph_fewshots import EXAMPLES
    from src.tools import graph_funcs, retriever
except ImportError as e:
    print(f"Warning: Could not import some modules: {e}")
    # ê¸°ë³¸ê°’ ì„¤ì •
    GRAPH_DEFINITION = {}
    EXAMPLES = {}
    enhanced_plan_generation_prompt = None

import logging
try:
    from transformers import pipeline, AutoTokenizer, AutoConfig, AutoModelForCausalLM
    import torch
except ImportError:
    print("Warning: transformers not available")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# IntegratedSolver í´ë˜ìŠ¤ (ë¬¸ë²• ì˜¤ë¥˜ ìˆ˜ì •)
class IntegratedSolver:
    """ëª¨ë“  ì§ˆë¬¸ ìœ í˜•ì„ í•´ê²°í•˜ëŠ” í†µí•© solver"""

    def __init__(self, graph_path=None):
        """ê·¸ë˜í”„ ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì¶•"""
        try:
            with open(graph_path, 'r') as f:
                self.graph = json.load(f)
                print(f"âœ… Graph loaded successfully from {graph_path}")
        except FileNotFoundError:
            print(f"âŒ Graph file not found: {graph_path}")
            self.graph = {}
        except json.JSONDecodeError:
            print(f"âŒ Invalid JSON in graph file: {graph_path}")
            self.graph = {}

        # ëª¨ë“  ì¸ë±ìŠ¤ êµ¬ì¶•
        self.build_indices()

    def build_indices(self):
        """íš¨ìœ¨ì ì¸ ê·¸ë˜í”„ íƒìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        print("ğŸ”¨ Building graph indices...")

        # ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self.disease_to_compounds = {}
        self.compound_to_classes = {}
        self.disease_upregulated_genes = {}
        self.disease_downregulated_genes = {}
        self.gene_to_cellular = {}
        self.gene_to_pathway = {}
        self.gene_to_biological_process = {}
        self.disease_to_symptoms = {}

        try:
            # Disease -> Compound ê´€ê³„ êµ¬ì¶•
            for disease_id, disease_data in self.graph.get('Disease_nodes', {}).items():
                neighbors = disease_data.get('neighbors', {})

                # Compound ê´€ê³„
                compounds = neighbors.get('Disease-treated_by-Compound', [])
                if compounds:
                    self.disease_to_compounds[disease_id] = set(compounds)

                # Gene ê´€ê³„
                upregulated = neighbors.get('Disease-upregulates-Gene', [])
                downregulated = neighbors.get('Disease-downregulates-Gene', [])

                if upregulated:
                    self.disease_upregulated_genes[disease_id] = set(upregulated)
                if downregulated:
                    self.disease_downregulated_genes[disease_id] = set(downregulated)

                # Symptom ê´€ê³„
                symptoms = neighbors.get('Disease-presents-Symptom', [])
                if symptoms:
                    self.disease_to_symptoms[disease_id] = set(symptoms)

            # Compound -> Pharmacologic Class ê´€ê³„ êµ¬ì¶•
            for compound_id, compound_data in self.graph.get('Compound_nodes', {}).items():
                neighbors = compound_data.get('neighbors', {})
                classes = neighbors.get('Pharmacologic Class-includes-Compound', [])  # ì—­ë°©í–¥
                if classes:
                    self.compound_to_classes[compound_id] = set(classes)

            # Gene -> Cellular Component/Pathway/Biological Process ê´€ê³„ êµ¬ì¶•
            for gene_id, gene_data in self.graph.get('Gene_nodes', {}).items():
                neighbors = gene_data.get('neighbors', {})

                cellular = neighbors.get('Gene-participates-Cellular Component', [])
                pathway = neighbors.get('Gene-participates-Pathway', [])
                biological = neighbors.get('Gene-participates-Biological Process', [])

                if cellular:
                    self.gene_to_cellular[gene_id] = set(cellular)
                if pathway:
                    self.gene_to_pathway[gene_id] = set(pathway)
                if biological:
                    self.gene_to_biological_process[gene_id] = set(biological)

            print(f"âœ… Indices built successfully:")
            print(f"  - Disease->Compound: {len(self.disease_to_compounds)} entries")
            print(f"  - Compound->Class: {len(self.compound_to_classes)} entries")
            print(f"  - Disease->Symptoms: {len(self.disease_to_symptoms)} entries")
            print(f"  - Gene->Cellular: {len(self.gene_to_cellular)} entries")

        except Exception as e:
            print(f"âŒ Error building indices: {e}")

    def find_entity_id(self, name: str, entity_types: List[str]) -> tuple:
        """ì—”í‹°í‹° ì´ë¦„ìœ¼ë¡œ ID ì°¾ê¸°"""
        if not name or not entity_types:
            return None, None

        name_lower = name.lower().strip()

        for entity_type in entity_types:
            nodes_key = f'{entity_type}_nodes'

            for node_id, node_data in self.graph.get(nodes_key, {}).items():
                node_name = node_data.get('features', {}).get('name', '').lower()

                # ì •í™•í•œ ë§¤ì¹­ ìš°ì„ 
                if name_lower == node_name:
                    return entity_type, node_id

                # ë¶€ë¶„ ë§¤ì¹­
                if name_lower in node_name or node_name in name_lower:
                    return entity_type, node_id

        return None, None


# GraphAgent í´ë˜ìŠ¤ (ë¬¸ë²• ì˜¤ë¥˜ ìˆ˜ì • ë° ì‹¤ì œ ê¸°ëŠ¥ êµ¬í˜„)
class GraphAgent:
    def __init__(self, args, agent_prompt=None) -> None:
        self.args = args
        self.max_steps = getattr(self.args, 'max_steps', 15)
        self.agent_prompt = agent_prompt

        # ì•ˆì „í•œ ì´ˆê¸°í™”
        try:
            self.examples = EXAMPLES.get(self.args.ref_dataset, []) if hasattr(self.args, 'ref_dataset') else []
        except:
            self.examples = []

        self.idd = []

        # Enhanced Graph Counselor attributes
        self.num_plans = getattr(self.args, 'num_plans', 5)
        self.top_plans = getattr(self.args, 'top_plans', 3)
        self.plan_scores = []
        self.answer_scores = []

        # Timing attributes ì´ˆê¸°í™”
        self.start_time = None
        self.end_time = None
        self.total_inference_time = 0.0
        self.plan_generation_time = 0.0
        self.plan_evaluation_time = 0.0
        self.execution_time = 0.0
        self.answer_evaluation_time = 0.0

        # Enhanced logging attributes
        self.detailed_plan_generation_log = []
        self.detailed_plan_evaluation_log = []
        self.detailed_execution_log = []
        self.detailed_answer_selection_log = []

        # File logging for results
        self.results_file = f"enhanced_graph_counselor_results_{getattr(self.args, 'dataset', 'unknown')}.txt"
        self.log_results_to_file = True

        # Plan Selection weights
        self.plan_weights = {
            'logical_coherence': 0.60,
            'relevance': 0.20,
            'completeness': 0.20
        }

        # Debug logging
        self.debug_log = []

        self.llm_version = getattr(self.args, 'llm_version', 'gpt-4o-mini')

        # API í‚¤ í™•ì¸ ë° ì„¤ì •
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key or 'your-api' in api_key:
            print("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # Initialize LLM (ì•ˆì „í•œ ì´ˆê¸°í™”)
        try:
            self.llm = ChatOpenAI(
                model=self.llm_version,
                temperature=0,
                openai_api_key=os.getenv('OPENAI_API_KEY')
            )

            # Initialize GPT-4 mini for plan evaluation
            self.gpt4_mini = ChatOpenAI(
                model="gpt-4o-mini",
                temperature=0,
                openai_api_key=os.getenv('OPENAI_API_KEY')
            )
        except Exception as e:
            print(f"âŒ LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            self.llm = None
            self.gpt4_mini = None

        # Load graph
        # Default to data/graph.json relative to project root if not specified
        default_graph_path = os.path.join(
            os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
            'data', 'graph.json'
        )
        self.graph_path = getattr(self.args, 'graph_dir', default_graph_path)
        self.load_graph()

        # Initialize graph functions (ì•ˆì „í•œ ì´ˆê¸°í™”)
        try:
            if hasattr(graph_funcs, 'graph_funcs'):
                self.graph_funcs = graph_funcs.graph_funcs(self.graph)
            else:
                print("âŒ graph_funcs ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                self.graph_funcs = None
        except Exception as e:
            print(f"âŒ graph_funcs ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            self.graph_funcs = None

        # Initialize retriever (ì•ˆì „í•œ ì´ˆê¸°í™”)
        try:
            if hasattr(retriever, 'Retriever'):
                self.retriever = retriever.Retriever(args, self.graph)
            else:
                print("âŒ retriever ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                self.retriever = None
        except Exception as e:
            print(f"âŒ retriever ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            self.retriever = None

        # Initialize IntegratedSolver
        try:
            self.integrated_solver = IntegratedSolver(graph_path=self.graph_path)

            # Make IntegratedSolver available to graph_funcs
            if self.graph_funcs:
                self.graph_funcs.integrated_solver = self.integrated_solver
        except Exception as e:
            print(f"âŒ IntegratedSolver ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            self.integrated_solver = None

        # Execution results storage
        self.execution_results = []

        # Initialize graph definition (ì•ˆì „í•œ ì´ˆê¸°í™”)
        try:
            dataset = getattr(self.args, 'dataset', 'default')
            self.graph_definition = GRAPH_DEFINITION.get(dataset, "") if GRAPH_DEFINITION else ""
        except:
            self.graph_definition = ""

        # Initialize encoder for token counting (ì•ˆì „í•œ ì´ˆê¸°í™”)
        self.init_encoder()

        # Reset agent state
        self.__reset_agent()

        print("âœ… Enhanced Graph Agent initialized successfully!")

    def init_encoder(self):
        """ì•ˆì „í•œ ì¸ì½”ë” ì´ˆê¸°í™”"""
        try:
            if self.llm_version in ['gpt-3.5-turbo', 'gpt-4', 'gpt-4o-mini', 'gpt-4-mini', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo-16k']:
                self.enc = tiktoken.encoding_for_model("gpt-4o-mini")
            elif self.llm_version in ['ERNIE-Speed-8K', 'ERNIE-Speed-128K', 'ERNIE-Lite-8K', 'ERNIE-Tiny-8K']:
                self.enc = tiktoken.encoding_for_model("text-davinci-003")
            else:
                self.enc = tiktoken.encoding_for_model("text-davinci-003")
        except Exception as e:
            print(f"Warning: Could not initialize encoder for {self.llm_version}: {e}")
            self.enc = tiktoken.encoding_for_model("text-davinci-003")

    def load_graph(self):
        """ì•ˆì „í•œ ê·¸ë˜í”„ ë¡œë”©"""
        try:
            logger.info('Loading the graph...')
            with open(self.graph_path, 'r') as f:
                self.graph = json.load(f)
            print(f"âœ… Graph loaded successfully: {len(self.graph)} node types")
        except FileNotFoundError:
            print(f"âŒ Graph file not found: {self.graph_path}")
            self.graph = {}
        except json.JSONDecodeError as e:
            print(f"âŒ Invalid JSON in graph file: {e}")
            self.graph = {}
        except Exception as e:
            print(f"âŒ Error loading graph: {e}")
            self.graph = {}

    def run(self, question, answer, reset=True, question_number=None) -> None:
        if reset:
            self.__reset_agent()
        self.question = question
        self.key = answer
        self.question_number = question_number or f"Q_{int(time.time())}"

        # ë””ë²„ê¹… ì •ë³´ ì´ˆê¸°í™”
        self.debug_log = []
        self.detailed_plan_generation_log = []
        self.detailed_plan_evaluation_log = []
        self.detailed_execution_log = []
        self.detailed_answer_selection_log = []

        self.debug_log.append(f"=== ENHANCED GRAPH COUNSELOR EXECUTION START ===")
        self.debug_log.append(f"Question: {question}")
        self.debug_log.append(f"Expected Answer: {answer}")
        self.debug_log.append(f"Question Number: {self.question_number}")
        self.debug_log.append(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # ì²« ë²ˆì§¸ ë‹µë³€ ì €ì¥
        if not hasattr(self, 'answer_first') or not self.answer_first:
            self.answer_first = self.answer

        # ì‹¤í–‰ ì‹œì‘ ì‹œê°„ ê¸°ë¡
        self.start_time = time.time()

        # Enhanced Graph Counselor ì‹¤í–‰ íë¦„ ì‚¬ìš©
        try:
            final_answer = self.execute_enhanced_graph_counselor(question, answer)
            self.answer = final_answer
        except Exception as e:
            self.debug_log.append(f"Enhanced Graph Counselor failed: {str(e)}")
            final_answer = self.generate_fallback_answer()
            self.answer = final_answer

        # ì‹¤í–‰ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
        self.end_time = time.time()
        self.total_inference_time = self.end_time - self.start_time

        # ìµœì¢… ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
        self.debug_log.append(f"=== EXECUTION END ===")
        self.debug_log.append(f"Final Answer: {self.answer}")
        self.debug_log.append(f"Total Steps: {self.step_n}")
        self.debug_log.append(f"Total Time: {self.total_inference_time:.2f}s")
        self.debug_log.append(f"Correct: {self.is_correct()}")

        # ê²°ê³¼ íŒŒì¼ì— ì €ì¥
        self.save_comprehensive_results()

    def execute_enhanced_graph_counselor(self, question, answer):
        """Enhanced Graph Counselor with comprehensive logging"""
        self.start_time = time.time()
        self.question = question
        self.key = answer

        print(f"\nğŸš€ Enhanced Graph Counselor ì‹œì‘: {question}")

        # 1. Plan Generation with detailed logging
        plan_generation_start = time.time()
        print(f"ğŸ“‹ 1ë‹¨ê³„: ê³„íš ìƒì„± ì‹œì‘...")
        plans = self.generate_multiple_plans_with_logging()
        self.plan_generation_time = time.time() - plan_generation_start

        if not plans:
            print("âŒ ê³„íš ìƒì„± ì‹¤íŒ¨ - fallback ë‹µë³€ ìƒì„±")
            return self.generate_fallback_answer()

        # 2. Plan Evaluation with detailed logging
        plan_evaluation_start = time.time()
        print(f"ğŸ” 2ë‹¨ê³„: ê³„íš í‰ê°€ ì‹œì‘...")
        selected_plans = self.evaluate_and_select_plans_with_logging(plans)
        self.plan_evaluation_time = time.time() - plan_evaluation_start

        if not selected_plans:
            print("âŒ ê³„íš ì„ íƒ ì‹¤íŒ¨ - fallback ë‹µë³€ ìƒì„±")
            return self.generate_fallback_answer()

        # 3. Plan Execution with detailed logging
        execution_start = time.time()
        print(f"âš¡ 3ë‹¨ê³„: ê³„íš ì‹¤í–‰ ì‹œì‘...")
        execution_results = self.execute_parallel_plans_with_logging(selected_plans)
        self.execution_time = time.time() - execution_start

        if not execution_results:
            print("âŒ ê³„íš ì‹¤í–‰ ì‹¤íŒ¨ - fallback ë‹µë³€ ìƒì„±")
            return self.generate_fallback_answer()

        # 4. Answer Evaluation with detailed logging
        answer_evaluation_start = time.time()
        print(f"ğŸ¯ 4ë‹¨ê³„: ë‹µë³€ í‰ê°€ ì‹œì‘...")
        final_answer = self.evaluate_and_select_answer_with_logging(execution_results)
        self.answer_evaluation_time = time.time() - answer_evaluation_start

        self.total_inference_time = time.time() - self.start_time

        print(f"âœ… ìµœì¢… ë‹µë³€: {final_answer}")

        return final_answer

    def generate_multiple_plans_with_logging(self):
        """Generate multiple plans with comprehensive logging"""
        plans = []
        self.detailed_plan_generation_log = []

        self.detailed_plan_generation_log.append({
            "phase": "plan_generation_start",
            "timestamp": datetime.now().isoformat(),
            "target_plans": self.num_plans,
            "methodology": {
                "approach": "Multi-step reasoning with graph traversal",
                "steps": ["Node identification", "Relationship mapping", "Path planning", "Answer synthesis"]
            }
        })

        print(f"ğŸ” ê³„íš ìƒì„± ì‹œì‘: {self.num_plans}ê°œ ê³„íš ìƒì„± ì˜ˆì •")

        for i in range(self.num_plans):
            plan_start_time = time.time()
            try:
                print(f"ğŸ“ ê³„íš {i+1} ìƒì„± ì¤‘...")
                plan = self.prompt_agent_for_plan(i)  # ê° ê³„íšë§ˆë‹¤ ë‹¤ë¥¸ ë³€í˜• ìƒì„±
                plan_generation_time = time.time() - plan_start_time

                plan_log = {
                    "plan_index": i + 1,
                    "generation_time": plan_generation_time,
                    "timestamp": datetime.now().isoformat(),
                    "status": "SUCCESS" if plan and plan.strip() else "EMPTY",
                    "plan_content": plan if plan else "",
                    "plan_length": len(plan.split()) if plan else 0,
                    "key_operations": self._extract_key_operations(plan) if plan else []
                }

                if plan and plan.strip():
                    plans.append((i, plan))
                    plan_log["result"] = "ACCEPTED"
                    print(f"âœ… ê³„íš {i+1} ìƒì„± ì„±ê³µ")
                else:
                    plan_log["result"] = "REJECTED"
                    plan_log["rejection_reason"] = "Empty or invalid plan"
                    print(f"âš ï¸ ê³„íš {i+1} ë¹ˆ ê²°ê³¼ - ê±´ë„ˆëœ€")

                self.detailed_plan_generation_log.append(plan_log)

            except Exception as e:
                plan_generation_time = time.time() - plan_start_time
                error_log = {
                    "plan_index": i + 1,
                    "generation_time": plan_generation_time,
                    "timestamp": datetime.now().isoformat(),
                    "status": "ERROR",
                    "error": str(e),
                    "error_type": type(e).__name__,
                    "result": "REJECTED",
                    "rejection_reason": f"Generation error: {str(e)}"
                }
                self.detailed_plan_generation_log.append(error_log)
                print(f"âŒ ê³„íš {i+1} ìƒì„± ì˜¤ë¥˜: {e}")

        summary_log = {
            "phase": "plan_generation_complete",
            "timestamp": datetime.now().isoformat(),
            "total_plans_attempted": self.num_plans,
            "successful_plans": len(plans),
            "failed_plans": self.num_plans - len(plans),
            "success_rate": len(plans) / self.num_plans if self.num_plans > 0 else 0
        }
        self.detailed_plan_generation_log.append(summary_log)

        print(f"ğŸ“Š ìµœì¢… ê³„íš ìƒì„± ê²°ê³¼: {len(plans)}ê°œ ê³„íš ìƒì„±ë¨")
        return plans

    def evaluate_and_select_plans_with_logging(self, plans):
        """Evaluate plans with comprehensive scoring and logging"""
        self.detailed_plan_evaluation_log = []

        self.detailed_plan_evaluation_log.append({
            "phase": "plan_evaluation_start",
            "timestamp": datetime.now().isoformat(),
            "total_plans": len(plans),
            "evaluation_criteria": {
                "logical_coherence": {"weight": 0.60, "description": "Plan structure and reasoning flow"},
                "relevance": {"weight": 0.20, "description": "Alignment with question requirements"},
                "completeness": {"weight": 0.20, "description": "Coverage of necessary steps"}
            }
        })

        print("ğŸ” ê³„íš í‰ê°€ ì‹œì‘...")

        # Evaluate all plans with detailed scoring
        evaluation_results = []
        for i, (plan_idx, plan) in enumerate(plans):
            evaluation_start_time = time.time()
            try:
                score, detailed_scoring = self.evaluate_plan_comprehensive(plan)
                evaluation_time = time.time() - evaluation_start_time

                evaluation_log = {
                    "plan_index": plan_idx + 1,
                    "evaluation_time": evaluation_time,
                    "timestamp": datetime.now().isoformat(),
                    "overall_score": score,
                    "rank": 0,  # Will be filled after sorting
                    "status": "EVALUATED",
                    "detailed_scoring": detailed_scoring,
                    "plan_content": plan,
                    "selection_status": "PENDING"
                }

                evaluation_results.append((plan_idx, score, plan, evaluation_log))
                print(f"ğŸ“Š ê³„íš {plan_idx+1} í‰ê°€ ì™„ë£Œ: {score:.2f}ì ")

            except Exception as e:
                evaluation_time = time.time() - evaluation_start_time
                error_log = {
                    "plan_index": plan_idx + 1,
                    "evaluation_time": evaluation_time,
                    "timestamp": datetime.now().isoformat(),
                    "status": "ERROR",
                    "error": str(e),
                    "error_type": type(e).__name__,
                    "overall_score": 3.0,  # Default score
                    "selection_status": "REJECTED"
                }
                evaluation_results.append((plan_idx, 3.0, plan, error_log))
                print(f"âŒ ê³„íš {plan_idx+1} í‰ê°€ ì˜¤ë¥˜: {e}")

        # Sort by score and assign ranks
        evaluation_results.sort(key=lambda x: x[1], reverse=True)

        # Update ranks and selection status
        for rank, (plan_idx, score, plan, log) in enumerate(evaluation_results):
            log["rank"] = rank + 1
            if rank < self.top_plans:
                log["selection_status"] = "SELECTED"
                log["selection_reason"] = f"Ranked #{rank+1} with score {score:.2f}"
            else:
                log["selection_status"] = "REJECTED"
                log["selection_reason"] = f"Ranked #{rank+1}, below top {self.top_plans} threshold"

            self.detailed_plan_evaluation_log.append(log)

        # Store plan scores for later use
        self.plan_scores = evaluation_results

        # Select top plans
        selected_plans = [plan for _, _, plan, _ in evaluation_results[:self.top_plans]]

        # Add summary
        summary_log = {
            "phase": "plan_evaluation_complete",
            "timestamp": datetime.now().isoformat(),
            "total_evaluated": len(evaluation_results),
            "selected_plans": len(selected_plans),
            "selection_criteria": f"Top {self.top_plans} by score",
            "score_distribution": {
                "highest": evaluation_results[0][1] if evaluation_results else 0,
                "lowest": evaluation_results[-1][1] if evaluation_results else 0,
                "average": sum(score for _, score, _, _ in evaluation_results) / len(evaluation_results) if evaluation_results else 0
            }
        }
        self.detailed_plan_evaluation_log.append(summary_log)

        print(f"ğŸ“Š ê³„íš í‰ê°€ ì™„ë£Œ: ìƒìœ„ {self.top_plans}ê°œ ê³„íš ì„ íƒë¨")
        return selected_plans

    def evaluate_plan_comprehensive(self, plan):
        """Comprehensive plan evaluation with SLC, SR, SC criteria"""
        detailed_scoring = {
            "logical_coherence_analysis": {},
            "relevance_analysis": {},
            "completeness_analysis": {},
            "total_breakdown": {}
        }

        # Use defined weights
        slc_weight = self.plan_weights['logical_coherence']  # 0.60
        sr_weight = self.plan_weights['relevance']           # 0.20
        sc_weight = self.plan_weights['completeness']        # 0.20

        # 1. ë…¼ë¦¬ì  ì¼ê´€ì„± (SLC) í‰ê°€ - 60% ê°€ì¤‘ì¹˜
        slc_score = self.evaluate_logical_coherence(plan)
        
        detailed_scoring["logical_coherence_analysis"] = {
            "score": slc_score,
            "weight": slc_weight,
            "weighted_score": slc_score * slc_weight,
            "description": "ì¶”ë¡  ë‹¨ê³„ ê°„ ë…¼ë¦¬ì  ì—°ê²°ì„±, ë°”ì´ì˜¤ë©”ë””ì»¬ ì¸ê³¼ê´€ê³„ ë…¼ë¦¬ ì¤€ìˆ˜"
        }

        # 2. ê´€ë ¨ì„± (SR) í‰ê°€ - 20% ê°€ì¤‘ì¹˜
        sr_score = self.evaluate_relevance(plan)
        
        detailed_scoring["relevance_analysis"] = {
            "score": sr_score,
            "weight": sr_weight,
            "weighted_score": sr_score * sr_weight,
            "description": "ì§ˆì˜ ìš”êµ¬ì‚¬í•­ê³¼ ê³„íš ë‚´ìš©ì˜ ì í•©ì„±, ë¶ˆí•„ìš”í•œ íƒìƒ‰ ê²½ë¡œì˜ ìµœì†Œí™”"
        }

        # 3. ì™„ì „ì„± (SC) í‰ê°€ - 20% ê°€ì¤‘ì¹˜
        sc_score = self.evaluate_completeness(plan)
        
        detailed_scoring["completeness_analysis"] = {
            "score": sc_score,
            "weight": sc_weight,
            "weighted_score": sc_score * sc_weight,
            "description": "ì§ˆì˜ í•´ê²°ì— í•„ìš”í•œ ëª¨ë“  ìš”ì†Œì˜ í¬í•¨ ì—¬ë¶€, ëˆ„ë½ëœ ì¤‘ìš” ë‹¨ê³„ ì‹ë³„"
        }

        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        final_score = (slc_score * slc_weight) + (sr_score * sr_weight) + (sc_score * sc_weight)

        # Total Score Breakdown
        detailed_scoring["total_breakdown"] = {
            "logical_coherence_score": slc_score,
            "logical_coherence_weight": slc_weight,
            "logical_coherence_contribution": slc_score * slc_weight,
            "relevance_score": sr_score,
            "relevance_weight": sr_weight,
            "relevance_contribution": sr_score * sr_weight,
            "completeness_score": sc_score,
            "completeness_weight": sc_weight,
            "completeness_contribution": sc_score * sc_weight,
            "final_score": min(5.0, final_score)
        }

        return min(5.0, final_score), detailed_scoring

    def evaluate_logical_coherence(self, plan):
        """ë…¼ë¦¬ì  ì¼ê´€ì„± í‰ê°€ (SLC) - ì¶”ë¡  ë‹¨ê³„ ê°„ ë…¼ë¦¬ì  ì—°ê²°ì„±"""
        plan_lower = plan.lower()
        score = 0.0
        reasons = []

        # 1. ë‹¨ê³„ë³„ ë…¼ë¦¬ì  íë¦„ (0.4ì )
        step_indicators = ['step', 'first', 'second', 'third', 'then', 'next', 'finally', 'after', 'before']
        step_count = sum(1 for indicator in step_indicators if indicator in plan_lower)
        if step_count >= 3:
            score += 0.4
            reasons.append("ëª…í™•í•œ ë‹¨ê³„ë³„ ë…¼ë¦¬ì  íë¦„")
        elif step_count >= 2:
            score += 0.3
            reasons.append("ê¸°ë³¸ì ì¸ ë‹¨ê³„ë³„ êµ¬ì¡°")
        elif step_count >= 1:
            score += 0.2
            reasons.append("ë¶€ë¶„ì ì¸ ë‹¨ê³„ êµ¬ì¡°")

        # 2. ë°”ì´ì˜¤ë©”ë””ì»¬ ì¸ê³¼ê´€ê³„ ë…¼ë¦¬ (0.3ì )
        causal_indicators = ['because', 'since', 'therefore', 'leads to', 'causes', 'results in', 'affects']
        causal_count = sum(1 for indicator in causal_indicators if indicator in plan_lower)
        if causal_count >= 2:
            score += 0.3
            reasons.append("ë°”ì´ì˜¤ë©”ë””ì»¬ ì¸ê³¼ê´€ê³„ ë…¼ë¦¬ ì¤€ìˆ˜")
        elif causal_count >= 1:
            score += 0.2
            reasons.append("ë¶€ë¶„ì ì¸ ì¸ê³¼ê´€ê³„ ë…¼ë¦¬")

        # 3. ê·¸ë˜í”„ íƒìƒ‰ ë…¼ë¦¬ (0.3ì )
        graph_ops = ['retrieve', 'neighbor', 'path', 'traversal', 'connect', 'link']
        graph_op_count = sum(1 for op in graph_ops if op in plan_lower)
        if graph_op_count >= 3:
            score += 0.3
            reasons.append("ì²´ê³„ì ì¸ ê·¸ë˜í”„ íƒìƒ‰ ë…¼ë¦¬")
        elif graph_op_count >= 2:
            score += 0.2
            reasons.append("ê¸°ë³¸ì ì¸ ê·¸ë˜í”„ íƒìƒ‰")
        elif graph_op_count >= 1:
            score += 0.1
            reasons.append("ë¶€ë¶„ì ì¸ ê·¸ë˜í”„ íƒìƒ‰")

        return min(1.0, score), reasons

    def evaluate_relevance(self, plan):
        """ê´€ë ¨ì„± í‰ê°€ (SR) - ì§ˆì˜ ìš”êµ¬ì‚¬í•­ê³¼ì˜ ì í•©ì„±"""
        plan_lower = plan.lower()
        question_lower = self.question.lower()
        score = 0.0
        reasons = []

        # 1. ì§ˆë¬¸ í‚¤ì›Œë“œ ë§¤ì¹­ (0.5ì )
        question_keywords = []
        if 'compound' in question_lower or 'drug' in question_lower:
            question_keywords.extend(['compound', 'drug', 'chemical', 'molecule'])
        if 'disease' in question_lower or 'condition' in question_lower:
            question_keywords.extend(['disease', 'condition', 'disorder', 'syndrome'])
        if 'side effect' in question_lower or 'adverse' in question_lower:
            question_keywords.extend(['side effect', 'adverse', 'reaction', 'toxicity'])
        if 'gene' in question_lower:
            question_keywords.extend(['gene', 'protein', 'mutation'])
        if 'pathway' in question_lower:
            question_keywords.extend(['pathway', 'process', 'mechanism'])

        matched_keywords = sum(1 for keyword in question_keywords if keyword in plan_lower)
        if matched_keywords >= 2:
            score += 0.5
            reasons.append("ì§ˆë¬¸ í‚¤ì›Œë“œì™€ ë†’ì€ ê´€ë ¨ì„±")
        elif matched_keywords >= 1:
            score += 0.3
            reasons.append("ì§ˆë¬¸ í‚¤ì›Œë“œì™€ ê¸°ë³¸ ê´€ë ¨ì„±")

        # 2. ë¶ˆí•„ìš”í•œ íƒìƒ‰ ê²½ë¡œ ìµœì†Œí™” (0.3ì )
        irrelevant_terms = ['unrelated', 'random', 'unnecessary', 'irrelevant']
        irrelevant_count = sum(1 for term in irrelevant_terms if term in plan_lower)
        if irrelevant_count == 0:
            score += 0.3
            reasons.append("ë¶ˆí•„ìš”í•œ íƒìƒ‰ ê²½ë¡œ ì—†ìŒ")
        elif irrelevant_count <= 1:
            score += 0.2
            reasons.append("ìµœì†Œí•œì˜ ë¶ˆí•„ìš”í•œ íƒìƒ‰")

        # 3. ì§ì ‘ì  í•´ê²° ê²½ë¡œ (0.2ì )
        direct_indicators = ['direct', 'straightforward', 'clear path', 'efficient']
        if any(indicator in plan_lower for indicator in direct_indicators):
            score += 0.2
            reasons.append("ì§ì ‘ì ì¸ í•´ê²° ê²½ë¡œ")

        return min(1.0, score), reasons

    def evaluate_completeness(self, plan):
        """ì™„ì „ì„± í‰ê°€ (SC) - í•„ìš”í•œ ëª¨ë“  ìš”ì†Œ í¬í•¨"""
        plan_lower = plan.lower()
        score = 0.0
        reasons = []

        # 1. í•µì‹¬ ì‘ì—… í¬í•¨ (0.4ì )
        core_operations = ['retrieve', 'search', 'find', 'analyze', 'finish']
        core_op_count = sum(1 for op in core_operations if op in plan_lower)
        if core_op_count >= 4:
            score += 0.4
            reasons.append("ëª¨ë“  í•µì‹¬ ì‘ì—… í¬í•¨")
        elif core_op_count >= 3:
            score += 0.3
            reasons.append("ëŒ€ë¶€ë¶„ì˜ í•µì‹¬ ì‘ì—… í¬í•¨")
        elif core_op_count >= 2:
            score += 0.2
            reasons.append("ê¸°ë³¸ì ì¸ í•µì‹¬ ì‘ì—… í¬í•¨")

        # 2. ë‹µë³€ í•©ì„± ë‹¨ê³„ (0.3ì )
        synthesis_indicators = ['finish', 'synthesize', 'conclude', 'answer', 'result']
        if any(indicator in plan_lower for indicator in synthesis_indicators):
            score += 0.3
            reasons.append("ë‹µë³€ í•©ì„± ë‹¨ê³„ í¬í•¨")

        # 3. ê²€ì¦ ë‹¨ê³„ (0.2ì )
        validation_indicators = ['verify', 'check', 'validate', 'confirm', 'test']
        if any(indicator in plan_lower for indicator in validation_indicators):
            score += 0.2
            reasons.append("ê²€ì¦ ë‹¨ê³„ í¬í•¨")

        # 4. ì˜¤ë¥˜ ì²˜ë¦¬ (0.1ì )
        error_handling = ['error', 'exception', 'fallback', 'alternative']
        if any(term in plan_lower for term in error_handling):
            score += 0.1
            reasons.append("ì˜¤ë¥˜ ì²˜ë¦¬ ê³ ë ¤")

        return min(1.0, score), reasons

    def execute_parallel_plans_with_logging(self, selected_plans):
        """Execute selected plans with comprehensive execution logging"""
        self.detailed_execution_log = []

        self.detailed_execution_log.append({
            "phase": "execution_start",
            "timestamp": datetime.now().isoformat(),
            "selected_plans_count": len(selected_plans),
            "execution_method": "Sequential (can be parallelized)",
            "max_steps_per_plan": self.max_steps
        })

        print("âš¡ ê³„íš ì‹¤í–‰ ì‹œì‘...")

        self.execution_results = []

        for i, plan in enumerate(selected_plans):
            execution_start_time = time.time()
            print(f"ğŸ”„ ê³„íš {i+1} ì‹¤í–‰ ì¤‘...")

            try:
                result = self.execute_single_plan_with_logging(plan, i + 1)
                execution_time = time.time() - execution_start_time

                result['execution_index'] = i + 1
                result['execution_time'] = execution_time
                result['timestamp'] = datetime.now().isoformat()

                self.execution_results.append(result)
                self.detailed_execution_log.append(result)

                print(f"âœ… ê³„íš {i+1} ì‹¤í–‰ ì™„ë£Œ: {result['answer']}")

            except Exception as e:
                execution_time = time.time() - execution_start_time
                error_result = {
                    'execution_index': i + 1,
                    'plan': plan,
                    'answer': 'EXECUTION_FAILED',
                    'scratchpad': f'Error: {e}',
                    'steps_taken': 0,
                    'execution_time': execution_time,
                    'timestamp': datetime.now().isoformat(),
                    'status': 'FAILED',
                    'error': str(e),
                    'error_type': type(e).__name__
                }
                self.execution_results.append(error_result)
                self.detailed_execution_log.append(error_result)
                print(f"âŒ ê³„íš {i+1} ì‹¤í–‰ ì‹¤íŒ¨: {e}")

        # Add execution summary
        successful_executions = [r for r in self.execution_results if r.get('status') != 'FAILED']
        failed_executions = [r for r in self.execution_results if r.get('status') == 'FAILED']

        summary_log = {
            "phase": "execution_complete",
            "timestamp": datetime.now().isoformat(),
            "total_executions": len(self.execution_results),
            "successful_executions": len(successful_executions),
            "failed_executions": len(failed_executions),
            "success_rate": len(successful_executions) / len(self.execution_results) if self.execution_results else 0,
            "average_steps": sum(r.get('steps_taken', 0) for r in successful_executions) / len(successful_executions) if successful_executions else 0
        }
        self.detailed_execution_log.append(summary_log)

        print(f"ğŸ“Š ì‹¤í–‰ ì™„ë£Œ: {len(successful_executions)}/{len(self.execution_results)} ì„±ê³µ")
        return self.execution_results

    def execute_single_plan_with_logging(self, plan, execution_index):
        """Execute a single plan with detailed step-by-step logging"""
        execution_log = {
            "execution_index": execution_index,
            "plan": plan,
            "status": "IN_PROGRESS",
            "step_details": [],
            "answer": "NO_ANSWER_GENERATED",
            "steps_taken": 0,
            "start_time": time.time()
        }

        try:
            # Reset agent state for this execution
            self.__reset_agent()
            self.scratchpad = f"Executing plan: {plan}\n"

            print(f"  ğŸ“ ê³„íš {execution_index} ì‹œì‘: {plan[:50]}...")

            # Execute the plan step by step with logging
            step_count = 0
            while not self.is_halted() and not self.is_finished() and step_count < self.max_steps:
                step_start_time = time.time()

                # Execute actual step using the real step method
                try:
                    step_result = self.execute_real_step(step_count, plan)
                except Exception as e:
                    step_result = {
                        'action': f'Error_Step_{step_count}',
                        'observation': f'Error: {str(e)}'
                    }

                step_duration = time.time() - step_start_time

                step_log = {
                    "step_number": step_count + 1,
                    "step_start_time": step_start_time,
                    "step_duration": step_duration,
                    "action": step_result.get('action', 'Unknown'),
                    "observation": step_result.get('observation', 'None'),
                    "answer_so_far": getattr(self, 'answer', 'None'),
                    "scratchpad_length": len(self.scratchpad),
                    "is_finished": self.is_finished(),
                    "is_halted": self.is_halted()
                }

                execution_log["step_details"].append(step_log)
                step_count += 1

                if self.is_finished():
                    break

            execution_log["steps_taken"] = step_count
            execution_log["answer"] = getattr(self, 'answer', 'NO_ANSWER_GENERATED')
            execution_log["scratchpad"] = self.scratchpad
            execution_log["status"] = "SUCCESS" if self.answer else "NO_ANSWER"
            execution_log["end_time"] = time.time()
            execution_log["total_duration"] = execution_log["end_time"] - execution_log["start_time"]

            print(f"  âœ… ê³„íš {execution_index} ì™„ë£Œ: {step_count} ìŠ¤í…, ë‹µë³€: {self.answer}")

            return execution_log

        except Exception as e:
            execution_log["status"] = "ERROR"
            execution_log["error"] = str(e)
            execution_log["error_type"] = type(e).__name__
            execution_log["end_time"] = time.time()
            execution_log["total_duration"] = execution_log["end_time"] - execution_log["start_time"]

            print(f"  âŒ ê³„íš {execution_index} ì˜¤ë¥˜: {e}")
            return execution_log

    def execute_real_step(self, step_count, plan):
        """Execute actual graph search step based on the plan - REAL IMPLEMENTATION"""
        try:
            # Parse the plan to understand what to do
            plan_lower = plan.lower()
            question_lower = self.question.lower()

            if step_count == 0:
                # First step: Use LLM to analyze question and extract entity
                entity_info = self.analyze_question_with_llm(self.question)
                if entity_info and entity_info.get('entity_name'):
                    entity_name = entity_info['entity_name']
                    entity_type = entity_info.get('entity_type', 'Compound')
                    entity_id = self.find_entity_in_graph_by_name(entity_name, entity_type)
                    
                    if entity_id:
                        self.current_entity_id = entity_id
                        self.current_entity_type = entity_type
                        self.scratchpad += f"Found {entity_type}: {entity_name} (ID: {entity_id})\n"
                        return {
                            'action': f'Retrieve[{entity_name}]',
                            'observation': f'Found {entity_type} ID: {entity_id}'
                        }
                    else:
                        return {
                            'action': f'Retrieve[{entity_name}]',
                            'observation': f'{entity_name} not found in graph'
                        }
                else:
                    return {
                        'action': 'Retrieve[Entity]',
                        'observation': 'Could not identify entity from question'
                    }

            elif step_count == 1:
                # Second step: Find relevant relationships
                if hasattr(self, 'current_entity_id') and self.current_entity_id:
                    relationships = self.get_entity_relationships(self.current_entity_id, self.current_entity_type)

                    # Use LLM to determine what we're looking for based on the question
                    target_relation = self.determine_target_relation_with_llm(self.question, self.current_entity_type)

                    if target_relation in relationships:
                        related_entities = relationships[target_relation]
                        self.current_related_entities = related_entities
                        self.scratchpad += f"Found {len(related_entities)} entities via {target_relation}\n"
                        return {
                            'action': f'Neighbor[{target_relation}]',
                            'observation': f'Found {len(related_entities)} related entities'
                        }
                    else:
                        return {
                            'action': f'Neighbor[{target_relation}]',
                            'observation': f'No {target_relation} relationships found'
                        }
                else:
                    return {
                        'action': 'Neighbor[Unknown]',
                        'observation': 'No entity ID available for relationship search'
                    }

            elif step_count == 2:
                # Third step: Extract and format the answer
                if hasattr(self, 'current_related_entities') and self.current_related_entities:
                    entity_names = self.get_entity_names(self.current_related_entities)
                    if entity_names:
                        self.answer = ', '.join(entity_names)
                        self.finished = True
                        self.scratchpad += f"Final answer: {self.answer}\n"
                        return {
                            'action': 'Finish[Answer]',
                            'observation': f'Final answer generated: {self.answer}'
                        }
                    else:
                        return {
                            'action': 'Finish[No_Names]',
                            'observation': 'Could not extract entity names'
                        }
                else:
                    return {
                        'action': 'Finish[No_Entities]',
                        'observation': 'No related entities found'
                    }

            else:
                # Additional steps if needed
                return {
                    'action': f'Additional_Step_{step_count}',
                    'observation': 'Processing additional information...'
                }

        except Exception as e:
            return {
                'action': f'Error_Step_{step_count}',
                'observation': f'Error: {str(e)}'
            }

    def analyze_question_with_llm(self, question):
        """Use LLM to analyze question and extract entity information"""
        try:
            # First try with OpenAI API if available
            if self.llm and hasattr(self.llm, 'model_name') and 'gpt' in self.llm.model_name.lower():
                analysis_prompt = f"""
                Analyze the following question and extract the main entity information:
                
                Question: {question}
                
                Please provide:
                1. The main entity name (e.g., compound name, disease name)
                2. The entity type (Compound, Disease, Gene, etc.)
                3. What information is being requested (side effects, treatments, etc.)
                
                Format your response as:
                Entity Name: [name]
                Entity Type: [type]
                Requested Info: [description]
                """
                
                response = self.llm.invoke([HumanMessage(content=analysis_prompt)])
                content = response.content
                
                # Parse the response
                entity_name = None
                entity_type = "Compound"  # default
                
                for line in content.split('\n'):
                    if 'Entity Name:' in line:
                        entity_name = line.split('Entity Name:')[1].strip()
                    elif 'Entity Type:' in line:
                        entity_type = line.split('Entity Type:')[1].strip()
                
                if entity_name:
                    return {
                        'entity_name': entity_name,
                        'entity_type': entity_type
                    }
            
            # Fallback: Simple pattern matching for common cases
            return self.analyze_question_fallback(question)
            
        except Exception as e:
            print(f"Error analyzing question with LLM: {e}")
            # Fallback to pattern matching
            return self.analyze_question_fallback(question)

    def analyze_question_fallback(self, question):
        """Fallback analysis using pattern matching"""
        question_lower = question.lower()
        
        # Extract compound name
        entity_name = None
        entity_type = "Compound"
        
        # Look for compound names in the question
        if "malathion" in question_lower:
            entity_name = "Malathion"
        elif "aspirin" in question_lower:
            entity_name = "Aspirin"
        elif "acetaminophen" in question_lower:
            entity_name = "Acetaminophen"
        elif "ibuprofen" in question_lower:
            entity_name = "Ibuprofen"
        elif "paracetamol" in question_lower:
            entity_name = "Paracetamol"
        
        # If no specific compound found, try to extract from "compound [name]" pattern
        if not entity_name:
            import re
            compound_match = re.search(r'compound\s+([a-zA-Z]+)', question_lower)
            if compound_match:
                entity_name = compound_match.group(1).capitalize()
        
        # Determine entity type based on question content
        if any(word in question_lower for word in ['disease', 'condition', 'disorder']):
            entity_type = "Disease"
        elif any(word in question_lower for word in ['gene', 'protein', 'mutation']):
            entity_type = "Gene"
        elif any(word in question_lower for word in ['pathway', 'process', 'mechanism']):
            entity_type = "Pathway"
        
        return {
            'entity_name': entity_name,
            'entity_type': entity_type
        }

    def determine_target_relation_with_llm(self, question, entity_type):
        """Use LLM to determine what type of relationship we're looking for"""
        try:
            # First try with OpenAI API if available
            if self.llm and hasattr(self.llm, 'model_name') and 'gpt' in self.llm.model_name.lower():
                relation_prompt = f"""
                Given this question: "{question}"
                And entity type: {entity_type}
                
                What type of relationship should we look for in the knowledge graph?
                
                Common relationship types:
                - Compound-causes-Side Effect (for side effects)
                - Compound-treats-Disease (for treatments)
                - Compound-binds-Gene (for gene interactions)
                - Disease-presents-Symptom (for symptoms)
                - Gene-participates-Pathway (for pathways)
                
                Respond with just the relationship type (e.g., "Compound-causes-Side Effect")
                """
                
                response = self.llm.invoke([HumanMessage(content=relation_prompt)])
                return response.content.strip()
            
            # Fallback: Pattern matching
            return self.determine_target_relation_fallback(question, entity_type)
            
        except Exception as e:
            print(f"Error determining relation with LLM: {e}")
            return self.determine_target_relation_fallback(question, entity_type)

    def determine_target_relation_fallback(self, question, entity_type):
        """Fallback relation determination using pattern matching"""
        question_lower = question.lower()
        
        if entity_type == "Compound":
            if any(word in question_lower for word in ['side effect', 'adverse', 'reaction', 'toxicity']):
                return "Compound-causes-Side Effect"
            elif any(word in question_lower for word in ['treat', 'therapy', 'medication']):
                return "Compound-treats-Disease"
            elif any(word in question_lower for word in ['gene', 'protein', 'interaction']):
                return "Compound-binds-Gene"
            else:
                return "Compound-causes-Side Effect"  # default for compounds
        elif entity_type == "Disease":
            if any(word in question_lower for word in ['symptom', 'present']):
                return "Disease-presents-Symptom"
            else:
                return "Disease-presents-Symptom"  # default for diseases
        elif entity_type == "Gene":
            if any(word in question_lower for word in ['pathway', 'process']):
                return "Gene-participates-Pathway"
            else:
                return "Gene-participates-Pathway"  # default for genes
        else:
            return "Compound-causes-Side Effect"  # general fallback

    def find_entity_in_graph_by_name(self, entity_name, entity_type):
        """Find entity in the graph by name and type"""
        try:
            if not hasattr(self, 'graph') or not self.graph:
                return None

            entity_name_lower = entity_name.lower()
            nodes_key = f'{entity_type}_nodes'
            
            if nodes_key not in self.graph:
                # Try to find in any node type
                for node_type, nodes in self.graph.items():
                    if not isinstance(nodes, dict):
                        continue
                    
                    for node_id, node_data in nodes.items():
                        features = node_data.get('features', {})
                        node_name = features.get('name', '').lower()
                        
                        if entity_name_lower == node_name:
                            return node_id
                        
                        # Partial match
                        if entity_name_lower in node_name or node_name in entity_name_lower:
                            return node_id
                return None
            
            # Search in specific node type
            nodes = self.graph.get(nodes_key, {})
            for node_id, node_data in nodes.items():
                features = node_data.get('features', {})
                node_name = features.get('name', '').lower()
                
                if entity_name_lower == node_name:
                    return node_id
                
                # Partial match
                if entity_name_lower in node_name or node_name in entity_name_lower:
                    return node_id

            return None

        except Exception as e:
            print(f"Error finding entity: {e}")
            return None

    def get_entity_relationships(self, entity_id, entity_type):
        """Get all relationships for an entity"""
        try:
            if not hasattr(self, 'graph') or not self.graph:
                return {}

            nodes_key = f'{entity_type}_nodes'
            entity_data = self.graph.get(nodes_key, {}).get(entity_id, {})
            return entity_data.get('neighbors', {})

        except Exception as e:
            print(f"Error getting relationships: {e}")
            return {}

    # Removed determine_target_relation method - now using LLM-based analysis

    def get_entity_names(self, entity_ids):
        """Get names of entities from their IDs"""
        try:
            if not hasattr(self, 'graph') or not self.graph:
                return []

            names = []

            # Search in all node types for the IDs
            for node_type, nodes in self.graph.items():
                if not isinstance(nodes, dict):
                    continue

                for entity_id in entity_ids:
                    if entity_id in nodes:
                        features = nodes[entity_id].get('features', {})
                        name = features.get('name', '')
                        if name and name not in names:
                            names.append(name)

            return names

        except Exception as e:
            print(f"Error getting entity names: {e}")
            return []

    def evaluate_and_select_answer_with_logging(self, execution_results):
        """Enhanced answer evaluation with consistent majority-based selection"""
        self.detailed_answer_selection_log = []

        self.detailed_answer_selection_log.append({
            "phase": "answer_evaluation_start",
            "timestamp": datetime.now().isoformat(),
            "total_executions": len(execution_results),
            "evaluation_method": "Consensus-based with consistency prioritization"
        })

        print("ğŸ¯ ë‹µë³€ í‰ê°€ ë° ì„ íƒ ì‹œì‘...")

        # Extract and analyze all answers
        answers = []
        answer_details = []

        for i, result in enumerate(execution_results):
            answer = result.get('answer', '')
            is_valid = answer and answer not in ['EXECUTION_FAILED', 'EXECUTION_ERROR', 'NO_ANSWER_GENERATED']

            answer_info = {
                "execution_index": i + 1,
                "answer": answer,
                "is_valid": is_valid,
                "steps_taken": result.get('steps_taken', 0),
                "execution_status": result.get('status', 'UNKNOWN'),
                "error": result.get('error', None)
            }

            if is_valid:
                answers.append(answer)

            answer_details.append(answer_info)

        # Perform consensus analysis
        if not answers:
            # No valid answers
            consensus_result = {
                "selection_method": "NO_VALID_ANSWERS",
                "selected_answer": "ALL_EXECUTIONS_FAILED",
                "confidence": 0.0,
                "reasoning": "All executions failed or produced invalid answers",
                "answer_distribution": {}
            }
        else:
            # Count occurrences of each answer
            answer_counts = Counter(answers)

            # Find the most common answer(s)
            max_count = max(answer_counts.values())
            most_common_answers = [ans for ans, count in answer_counts.items() if count == max_count]

            if len(most_common_answers) == 1:
                # Clear majority
                selected_answer = most_common_answers[0]
                confidence = max_count / len(answers)
                selection_method = "MAJORITY_CONSENSUS"
                reasoning = f"Answer '{selected_answer}' appeared in {max_count}/{len(answers)} executions"
            else:
                # Tie - select first valid answer (can be enhanced with other criteria)
                selected_answer = answers[0]
                confidence = 1.0 / len(most_common_answers)
                selection_method = "TIE_BREAKER"
                reasoning = f"Multiple answers tied with {max_count} occurrences, selected first valid answer"

            consensus_result = {
                "selection_method": selection_method,
                "selected_answer": selected_answer,
                "confidence": confidence,
                "reasoning": reasoning,
                "answer_distribution": dict(answer_counts),
                "total_valid_answers": len(answers),
                "unique_answers": len(answer_counts)
            }

        # Enhanced consistent judgment logic
        final_selected_answer = consensus_result["selected_answer"]

        # Update answer details with consistent judgment
        for answer_info in answer_details:
            answer = answer_info["answer"]

            if not answer_info["is_valid"]:
                # Invalid answers are always rejected
                answer_info["judgment"] = "NO"
                answer_info["selection_status"] = "REJECTED"
                answer_info["selection_reason"] = "Invalid or empty answer"
            elif answer == final_selected_answer:
                # All instances of the selected answer are marked as selected
                answer_info["judgment"] = "YES"
                answer_info["selection_status"] = "SELECTED"
                answer_info["selection_reason"] = f"Matches consensus answer: {consensus_result['reasoning']}"
            else:
                # Valid but not selected answers
                answer_info["judgment"] = "NO"
                answer_info["selection_status"] = "REJECTED"
                answer_info["selection_reason"] = f"Valid answer but not selected (consensus chose '{final_selected_answer}')"

        # Log detailed analysis
        analysis_log = {
            "phase": "answer_analysis",
            "timestamp": datetime.now().isoformat(),
            "consensus_result": consensus_result,
            "answer_evaluations": answer_details
        }
        self.detailed_answer_selection_log.append(analysis_log)

        # Final selection log
        selection_log = {
            "phase": "answer_selection_complete",
            "timestamp": datetime.now().isoformat(),
            "final_answer": final_selected_answer,
            "selection_confidence": consensus_result["confidence"],
            "selection_method": consensus_result["selection_method"],
            "total_selected": len([a for a in answer_details if a["selection_status"] == "SELECTED"]),
            "total_rejected": len([a for a in answer_details if a["selection_status"] == "REJECTED"])
        }
        self.detailed_answer_selection_log.append(selection_log)

        # Set the final answer
        self.answer = final_selected_answer

        print(f"ğŸ¯ ë‹µë³€ ì„ íƒ ì™„ë£Œ: '{final_selected_answer}' (ì‹ ë¢°ë„: {consensus_result['confidence']:.2f})")

        return final_selected_answer

    def save_comprehensive_results(self):
        """Save comprehensive results to JSON file with timestamp"""
        try:
            # Create results directory
            results_dir = "enhanced_counselor_results"
            os.makedirs(results_dir, exist_ok=True)

            # Generate filename with timestamp
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            question_safe = re.sub(r'[^\w\s-]', '', str(self.question_number)).strip()[:20]
            filename = f"enhanced_graph_counselor_{timestamp}_{question_safe}.json"
            filepath = os.path.join(results_dir, filename)

            # Prepare comprehensive results data
            results_data = {
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "question_number": self.question_number,
                    "model": self.llm_version,
                    "dataset": getattr(self.args, 'dataset', 'unknown'),
                    "max_steps": self.max_steps,
                    "num_plans": self.num_plans,
                    "top_plans": self.top_plans,
                    "graph_schema": self._extract_graph_schema()
                },
                "question_analysis": {
                    "question": self.question,
                    "expected_answer": self.key,
                    "predicted_answer": self.answer,
                    "answer_match": "CORRECT" if self.is_correct() else "INCORRECT",
                    "final_judgment": "YES" if self.answer and self.answer != "ALL_EXECUTIONS_FAILED" else "NO"
                },
                "timing_information": {
                    "total_inference_time": self.total_inference_time,
                    "plan_generation_time": self.plan_generation_time,
                    "plan_evaluation_time": self.plan_evaluation_time,
                    "execution_time": self.execution_time,
                    "answer_evaluation_time": self.answer_evaluation_time,
                    "breakdown_percentages": {
                        "plan_generation": (self.plan_generation_time / self.total_inference_time * 100) if self.total_inference_time > 0 else 0,
                        "plan_evaluation": (self.plan_evaluation_time / self.total_inference_time * 100) if self.total_inference_time > 0 else 0,
                        "execution": (self.execution_time / self.total_inference_time * 100) if self.total_inference_time > 0 else 0,
                        "answer_evaluation": (self.answer_evaluation_time / self.total_inference_time * 100) if self.total_inference_time > 0 else 0
                    }
                },
                "plan_generation_details": {
                    "methodology": {
                        "approach": "Multi-step reasoning with graph traversal",
                        "steps": ["Node identification", "Relationship mapping", "Path planning", "Answer synthesis"],
                        "generated_plans": self.num_plans,
                        "selected_top": self.top_plans
                    },
                    "detailed_log": self.detailed_plan_generation_log
                },
                "plan_evaluation_details": {
                    "evaluation_criteria": {
                        "logical_coherence": {"weight": 0.60, "description": "Plan structure and reasoning flow"},
                        "relevance": {"weight": 0.20, "description": "Alignment with question requirements"},
                        "completeness": {"weight": 0.20, "description": "Coverage of necessary steps"}
                    },
                    "detailed_log": self.detailed_plan_evaluation_log
                },
                "execution_details": {
                    "execution_method": "Sequential execution of top-ranked plans with REAL graph search",
                    "max_steps_per_plan": self.max_steps,
                    "detailed_log": self.detailed_execution_log
                },
                "answer_selection_details": {
                    "selection_method": "Consensus-based with consistency prioritization",
                    "detailed_log": self.detailed_answer_selection_log
                },
                "system_configuration": {
                    "model": self.llm_version,
                    "dataset": getattr(self.args, 'dataset', 'unknown'),
                    "max_steps": self.max_steps,
                    "plan_selection_weights": self.plan_weights
                },
                "reflection_based_replanning": {
                    "triggered": False,
                    "reason": "Answer quality was acceptable" if self.answer and self.answer != "ALL_EXECUTIONS_FAILED" else "All executions failed"
                },
                "debug_log": self.debug_log
            }

            # Save to JSON file
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results_data, f, indent=2, ensure_ascii=False)

            print(f"âœ… ì¢…í•© ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")
            return filepath

        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _extract_graph_schema(self):
        """Extract graph schema information"""
        try:
            schema = {
                "entity_types": list(self.graph.keys()) if hasattr(self, 'graph') and self.graph else [],
                "total_nodes": sum(len(nodes) for nodes in self.graph.values()) if hasattr(self, 'graph') and self.graph else 0
            }

            # Add relation types if available
            if hasattr(self, 'graph') and self.graph:
                relation_types = set()
                for node_type, nodes in self.graph.items():
                    if isinstance(nodes, dict):
                        for node_id, node_data in nodes.items():
                            if 'neighbors' in node_data:
                                relation_types.update(node_data['neighbors'].keys())
                schema["relation_types"] = list(relation_types)

            return schema

        except Exception as e:
            return {"error": f"Could not extract schema: {e}"}

    def _extract_key_operations(self, plan):
        """Extract key operations from plan text"""
        if not plan:
            return []

        key_ops = []
        plan_lower = plan.lower()

        operations = ['retrieve', 'feature', 'neighbor', 'degree', 'finish', 'search', 
                     'find', 'analyze', 'compare', 'identify']

        for op in operations:
            if op in plan_lower:
                key_ops.append(op)

        return key_ops

    def generate_fallback_answer(self):
        """Generate a fallback answer when execution fails"""
        return "Unable to determine answer"

    def prompt_agent_for_plan(self, plan_index=0):
        """Generate a plan using variations for different approaches"""
        try:
            # Generate different plan variations
            plan_templates = [
                f"Step 1: Retrieve the main entity from question: {self.question}\nStep 2: Find all relevant relationships using graph traversal\nStep 3: Extract target information and format as answer\nStep 4: Finish with complete answer",
                f"First, identify the compound or entity mentioned. Then search for its side effects or related information in the knowledge graph. Finally, compile and return the results.",
                f"1. Parse question to find target entity\n2. Search graph for entity node\n3. Traverse relationships to find answer\n4. Format and return results",
                f"Begin by locating the entity in the graph. Next, explore its connections to find relevant information. Then synthesize the findings into a comprehensive answer.",
                f"Step 1: Entity extraction and identification\nStep 2: Graph neighborhood exploration\nStep 3: Information compilation\nStep 4: Answer synthesis and formatting"
            ]

            if plan_index < len(plan_templates):
                return plan_templates[plan_index]
            else:
                return plan_templates[plan_index % len(plan_templates)]

        except Exception as e:
            print(f"Error generating plan: {e}")
            return None

    def __reset_agent(self) -> None:
        """ì—ì´ì „íŠ¸ ìƒíƒœ ì´ˆê¸°í™”"""
        self.step_n = 1
        self.answer = ''
        if not hasattr(self, 'answer_first'):
            self.answer_first = ''
        self.finished = False
        self.scratchpad = ''
        self.idd = []

        # Reset Enhanced Graph Counselor attributes
        self.plans = []
        self.selected_plans = []
        self.plan_scores = []
        self.execution_results = []
        self.answer_scores = []

    # Helper methods
    def is_finished(self) -> bool:
        return getattr(self, 'finished', False)

    def is_correct(self) -> bool:
        try:
            return EM(getattr(self, 'answer', ''), getattr(self, 'key', ''))
        except:
            return False

    def is_halted(self) -> bool:
        try:
            return ((self.step_n > self.max_steps) or 
                   (self.enc and len(self.enc.encode(str(self.question))) > 10000)) and not self.finished
        except:
            return self.step_n > self.max_steps and not self.finished


# Helper functions
def normalize_answer(s):
    def remove_articles(text):
        return re.sub(r"\b(a|an|the|usd)\b", " ", text)

    def white_space_fix(text):
        return " ".join(text.split())

    def remove_punc(text):
        exclude = set(string.punctuation)
        return "".join(ch for ch in text if ch not in exclude)

    def lower(text):
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))

def EM(answer, key) -> bool:
    try:
        return normalize_answer(str(answer)) == normalize_answer(str(key))
    except:
        return False

def gpt_format_step(step: str) -> str:
    try:
        return step.content.strip('\n').strip().replace('\n', '')
    except:
        return str(step).strip('\n').strip().replace('\n', '')


# Test class for quick testing
class MockArgs:
    def __init__(self):
        self.max_steps = 15
        self.num_plans = 3
        self.top_plans = 2
        self.llm_version = 'gpt-4o-mini'
        self.dataset = 'test'
        # Default graph path relative to project root
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        self.graph_dir = os.path.join(project_root, 'data', 'graph.json')


# Enhanced Graph Counselor - Production Ready
if __name__ == "__main__":
    print("Enhanced Graph Counselor - Production Mode with Real Graph Search")
    print("=" * 70)

    # Test the system
    try:
        args = MockArgs()
        agent = GraphAgent(args)

        # Test with Malathion question
        test_question = "What are the side effects of Malathion?"
        test_answer = "Conjunctivitis, Sensitisation, Chemical burn, Burns second degree, Chemical injury"

        print(f"\nğŸ§ª Testing with question: {test_question}")
        agent.run(test_question, test_answer, question_number="TEST_001")

        print(f"\nğŸ“Š Test Results:")
        print(f"  Question: {test_question}")
        print(f"  Expected: {test_answer}")
        print(f"  Predicted: {agent.answer}")
        print(f"  Correct: {agent.is_correct()}")
        print(f"  Total Time: {agent.total_inference_time:.2f}s")

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
